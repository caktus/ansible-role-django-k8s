---
# IMPORTANT!
# ansible-role-django-k8s performs a two-step deploy process.
# First, it uses your (admin) credentials to create a namespace and service account;
# those admin credentials should be defined with a combination of these variables:

# Next, after the service account is created, it obtains the authentication token
# from the server and uses that to create the remaining resources in the namespace.
# This allows you to store an encrypted copy of k8s_auth_api_key in version control
# for continous deployment from your CI/CD service, with access only to this namespace.
#
# You'll need to set your k8s_auth_host (Kubernetes API server) and k8s_auth_ssl_ca_cert
# (certificate authority) appropriately; you can obtain these from your ~/.kube/config
# (they should also be stored in the repo, but don't need to be encrypted).
k8s_auth_api_key: ""
k8s_auth_host: ""
k8s_auth_ssl_ca_cert: ""  # should be a file path (relative to deploy.yaml)

# An empty dictionary, or the Python representation of your ~/.docker/config.json,
# which can easily be generated with Ansible. For example, to provide credentials
# for Docker Hub:
# docker_hub_user: myusername
# docker_hub_password: mysupersecretpass
# k8s_dockerconfigjson:
#   auths:
#     https://index.docker.io/v1/:
#       auth: "{{ [docker_hub_user, docker_hub_password] | join(':') | b64encode }}"
k8s_dockerconfigjson: {}

# Whether to do a "kubectl rollout restart" on each deploy.
# This should be unnecessary if deploys are using unique image tags,
# but might be desired if a project is using tags like 'latest'
# or 'staging' that will refer to different tags at different times,
# to make sure the latest version of the image with that tag gets
# pulled and used. When setting this 'true', ensure you also set
# `k8s_container_image_pull_policy: Always`
k8s_rollout_after_deploy: False

k8s_domain_names:
  - www.example.com
# In some scenarios is required to redirect from www.domain.com to
# domain.com or vice versa (feature of NGINX Ingress Controller).
#   Note: If enabled, the redirected domain must be manually added
#   to the k8s_ingress_tls_domains_extra variable to create a
#   corresponding TLS certificate, e.g,:
#     k8s_ingress_tls_domains_extra: [example.com]
#   https://kubernetes.github.io/ingress-nginx/user-guide/nginx-configuration/annotations/#redirect-fromto-www
k8s_domain_from_to_www_redirect: false

k8s_namespace: "echoserver"

# Optional: Set the storageClassName used for any PersistentVolumeClaims (PVCs) used in
# templates in this role (including Redis, RabbitMQ, Elasticsearch, and Celery Beat).
# Warning: Changing this value after resources are created is unsupported. Instead, you will need to
# delete and recreate any pods that depend on it (assuming that is actually safe to do). You'll get a
# "spec is immutable after creation" error if you do attempt to change it).
# k8s_storage_class_name: ""

k8s_memcached_enabled: false
k8s_memcached_version: "1.6.9"
k8s_memcached_service_type: ClusterIP
# If service_type is LoadBalancer, you can optionally assign a fixed IP for your
# load balancer (if suppported by the provider):
# k8s_memcached_load_balancer_ip: w.x.y.z

k8s_redis_enabled: false
k8s_redis_version: "5.0.6"
k8s_redis_volume_size: "20Gi"
k8s_redis_service_type: ClusterIP
# If service_type is LoadBalancer, you can optionally assign a fixed IP for your
# load balancer (if suppported by the provider):
# k8s_redis_load_balancer_ip: w.x.y.z

k8s_elasticsearch_enabled: false
k8s_elasticsearch_cluster_name: "app-elasticsearch-cluster"
k8s_elasticsearch_version: "7.5.2"
k8s_elasticsearch_volume_size: "20Gi"
k8s_elasticsearch_node_replicas: 3
k8s_elasticsearch_resources:
  requests:
    memory: "1Gi"

# NOTE: Using RabbitMQ relies on the RabbitMQ Cluster Kubernetes Operator.
# Follow the instructions in the README to install and configure it before
# attempting to create a RabbitMQ cluster. The Operator also controls the
# version of RabbitMQ that is installed (support for customizing spec.image
# could be considered for the future, if needed).
k8s_rabbitmq_enabled: false
k8s_rabbitmq_cluster_name: rabbitmq
# Using odd numbers is "highly recommended," and reducing this number ("cluster
# scale down") is not supported.
# See: https://www.rabbitmq.com/kubernetes/operator/using-operator.html#update
k8s_rabbitmq_replicas: 1
# Important: Updating the volume size after cluster creation does not appear
# to be supported by the Operator (as of v1.9.0 at least). You'll need to
# delete and recreate the cluster (by setting k8s_rabbitmq_enabled to false
# temporarily) to effect a change in the volume size.
k8s_rabbitmq_volume_size: "20Gi"
k8s_rabbitmq_service_type: ClusterIP
# If service_type is LoadBalancer, you can optionally assign a fixed IP for your
# load balancer (if suppported by the provider):
# k8s_rabbitmq_load_balancer_ip: w.x.y.z

k8s_environment_variables: {}

k8s_ingress_certificate_issuer: letsencrypt-production
# user-provided additional domain names to be associated with
# TLS certificate as Subject Alternative Name(s)
k8s_ingress_tls_domains_extra: []
k8s_ingress_tls_domains: "{{ k8s_domain_names + k8s_ingress_tls_domains_extra }}"
k8s_ingress_class: nginx

k8s_container_name: web
k8s_container_image: k8s.gcr.io/echoserver
k8s_container_image_pull_policy: IfNotPresent
k8s_container_image_tag: "1.10"
k8s_container_port: 8080
k8s_container_protocol: http
k8s_container_replicas: 2
k8s_container_resources:
  requests:
    memory: "256Mi"
    cpu: "250m"
k8s_container_ingress_annotations: {}
k8s_container_ingress_path_type: Prefix
k8s_container_ingress_paths:
  - /
# An empty string, or the contents of a file generated by the `htpasswd` command
k8s_container_htpasswd: ""

k8s_web_containers:
  - name: "{{ k8s_container_name }}"
    image: "{{ k8s_container_image }}"
    image_pull_policy: "{{ k8s_container_image_pull_policy }}"
    tag: "{{ k8s_container_image_tag }}"
    replicas: "{{ k8s_container_replicas }}"
    resources: "{{ k8s_container_resources }}"
    port: "{{ k8s_container_port }}"
    protocol: "{{ k8s_container_protocol }}"
    ingress_annotations: "{{ k8s_container_ingress_annotations }}"
    ingress_paths: "{{ k8s_container_ingress_paths }}"
    environment: "{{ k8s_environment_variables }}"
    htpasswd: "{{ k8s_container_htpasswd }}"

k8s_worker_enabled: false
k8s_worker_name: "celery-worker"
k8s_worker_image: "{{ k8s_container_image }}"
k8s_worker_image_pull_policy: "{{ k8s_container_image_pull_policy }}"
k8s_worker_image_tag: "{{ k8s_container_image_tag }}"
k8s_worker_replicas: 2
k8s_worker_resources: "{{ k8s_container_resources }}"
k8s_worker_celery_app: ""
k8s_worker_command:
  - celery
  - --app={{ k8s_worker_celery_app }}
  - worker
  - --workdir=/code
  - --loglevel=info
k8s_worker_beat_command:
  - celery
  - --app={{ k8s_worker_celery_app }}
  - beat
  - --workdir=/code
  - --loglevel=info
  - --pidfile=/data/beat.pid
  - --schedule=/data/schedulefile.db
k8s_worker_container:
  name: "{{ k8s_worker_name }}"
  image: "{{ k8s_worker_image }}"
  image_pull_policy: "{{ k8s_worker_image_pull_policy }}"
  tag: "{{ k8s_worker_image_tag }}"
  replicas: "{{ k8s_worker_replicas }}"
  resources: "{{ k8s_worker_resources }}"
  celery_app: "{{ k8s_worker_celery_app }}"
  environment: "{{ k8s_environment_variables }}"
  worker_command: "{{ k8s_worker_command }}"
  beat_command: "{{ k8s_worker_beat_command }}"
# Override k8s_worker_containers to add additional workers (background tasks):
k8s_worker_containers:
  - "{{ k8s_worker_container }}"

k8s_worker_beat_enabled: false

k8s_migrations_enabled: true
k8s_migration_command:
  - python
  - manage.py
  - migrate
  - --noinput
  - -v
  - "2"

k8s_collectstatic_enabled: true
k8s_collectstatic_timeout: 120  # seconds
k8s_collectstatic_command:
  - python
  - manage.py
  - collectstatic
  - --noinput
  - -v
  - "2"

k8s_batchjob_state: "{{ (k8s_migrations_enabled or k8s_collectstatic_enabled) | ternary('present', 'absent') }}"

k8s_s3_cluster_name: ""  # name of EKS cluster in AWS
k8s_s3_region: "us-east-1"
k8s_s3_namespace: "{{ k8s_namespace }}"
k8s_s3_iam_role: "S3ServiceAccountRole-{{ k8s_s3_namespace }}"
k8s_s3_serviceaccount: "default"
k8s_s3_public_bucket: "{{ k8s_s3_namespace }}-assets"
k8s_s3_private_bucket: "{{ k8s_s3_namespace }}-private-assets"
k8s_s3_tags: {}
k8s_s3_tags_purge: yes

k8s_templates:
  - name: registry_secret.yaml.j2
    state: "{{ k8s_dockerconfigjson | ternary('present', 'absent') }}"
  - name: redis.yaml.j2
    state: "{{ k8s_redis_enabled | ternary('present', 'absent') }}"
  - name: elasticsearch.yaml.j2
    state: "{{ k8s_elasticsearch_enabled | ternary('present', 'absent') }}"
  - name: memcached.yaml.j2
    state: "{{ k8s_memcached_enabled | ternary('present', 'absent') }}"
  - name: rabbitmq.yaml.j2
    state: "{{ k8s_rabbitmq_enabled | ternary('present', 'absent') }}"
  - name: web.yaml.j2
    state: present
  - name: workers.yaml.j2
    state: "{{ k8s_worker_enabled | ternary('present', 'absent') }}"
  - name: workers_beat.yaml.j2
    state: "{{ (k8s_worker_enabled and k8s_worker_beat_enabled) | ternary('present', 'absent') }}"


# Note that this is only supported on AWS clusters
k8s_ci_create_user: "{{ k8s_ci_repository_arn | length > 0 }}"
k8s_ci_username: ci-user
k8s_ci_repository_arn: "" # format: arn:aws:ecr:<REGION>:<ACCOUNT_NUMBER>:repository/<REPO_NAME>
k8s_ci_vault_password_arn: "" # format: arn:aws:secretsmanager:<REGION>:<ACCOUNT_NUMBER:secret:<SECRET_IDENTIFIER>
