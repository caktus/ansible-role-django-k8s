---
# IMPORTANT!
# ansible-role-django-k8s performs a two-step deploy process.
# First, it uses your (admin) credentials to create a namespace and service account;
# those admin credentials should be defined with a combination of these variables:

# Next, after the service account is created, it obtains the authentication token
# from the server and uses that to create the remaining resources in the namespace.
# This allows you to store an encrypted copy of k8s_auth_api_key in version control
# for continous deployment from your CI/CD service, with access only to this namespace.
#
# You'll need to set your k8s_auth_host (Kubernetes API server) and k8s_auth_ssl_ca_cert
# (certificate authority) appropriately; you can obtain these from your ~/.kube/config
# (they should also be stored in the repo, but don't need to be encrypted).
k8s_auth_api_key: ""
k8s_auth_host: ""
k8s_auth_ssl_ca_cert: ""  # should be a file path (relative to deploy.yaml)

# An empty dictionary, or the Python representation of your ~/.docker/config.json,
# which can easily be generated with Ansible. For example, to provide credentials
# for Docker Hub:
# docker_hub_user: myusername
# docker_hub_password: mysupersecretpass
# k8s_dockerconfigjson:
#   auths:
#     https://index.docker.io/v1/:
#       auth: "{{ [docker_hub_user, docker_hub_password] | join(':') | b64encode }}"
k8s_dockerconfigjson: {}

# Whether to do a "kubectl rollout restart" on each deploy.
# This should be unnecessary if deploys are using unique image tags,
# but might be desired if a project is using tags like 'latest'
# or 'staging' that will refer to different tags at different times,
# to make sure the latest version of the image with that tag gets
# pulled and used. When setting this 'true', ensure you also set
# `k8s_container_image_pull_policy: Always`
k8s_rollout_after_deploy: False

k8s_domain_names:
  - www.example.com
# In some scenarios is required to redirect from www.domain.com to
# domain.com or vice versa (feature of NGINX Ingress Controller).
#   Note: If enabled, the redirected domain must be manually added
#   to the k8s_ingress_tls_domains_extra variable to create a
#   corresponding TLS certificate, e.g,:
#     k8s_ingress_tls_domains_extra: [example.com]
#   https://kubernetes.github.io/ingress-nginx/user-guide/nginx-configuration/annotations/#redirect-fromto-www
k8s_domain_from_to_www_redirect: false

k8s_namespace: "echoserver"

# Optional: Set the storageClassName used for any PersistentVolumeClaims (PVCs) used in
# templates in this role (including Redis, Elasticsearch, and Celery Beat). Warning:
# Changing this value after resources are created is unsupported. Instead, you will need to
# delete and recreate any pods that depend on it (assuming that is actually safe to do). You'll
# get a "spec is immutable after creation" error if you do attempt to change it).
# k8s_storage_class_name: ""

k8s_memcached_enabled: false
k8s_memcached_version: "1.6.9"
k8s_memcached_service_type: ClusterIP
# If service_type is LoadBalancer, you can optionally assign a fixed IP for your
# load balancer (if suppported by the provider):
# k8s_memcached_load_balancer_ip: w.x.y.z

k8s_redis_enabled: false
k8s_redis_version: "5.0.6"
k8s_redis_volume_size: "20Gi"
k8s_redis_service_type: ClusterIP
# If service_type is LoadBalancer, you can optionally assign a fixed IP for your
# load balancer (if suppported by the provider):
# k8s_redis_load_balancer_ip: w.x.y.z

k8s_elasticsearch_enabled: false
k8s_elasticsearch_cluster_name: "app-elasticsearch-cluster"
k8s_elasticsearch_version: "7.5.2"
k8s_elasticsearch_volume_size: "20Gi"
k8s_elasticsearch_node_replicas: 3
k8s_elasticsearch_resources:
  requests:
    memory: "1Gi"

k8s_environment_variables: {}

k8s_ingress_certificate_issuer: letsencrypt-production
# user-provided additional domain names to be associated with
# TLS certificate as Subject Alternative Name(s)
k8s_ingress_tls_domains_extra: []
k8s_ingress_tls_domains: "{{ k8s_domain_names + k8s_ingress_tls_domains_extra }}"
k8s_ingress_class: nginx

k8s_container_name: web
k8s_container_image: k8s.gcr.io/echoserver
k8s_container_image_pull_policy: IfNotPresent
k8s_container_image_tag: "1.10"
k8s_container_port: 8080
k8s_container_protocol: http
k8s_container_replicas: 2
k8s_container_resources:
  requests:
    memory: "256Mi"
    cpu: "250m"
k8s_container_ingress_annotations: {}
k8s_container_ingress_path_type: Prefix
k8s_container_ingress_paths:
  - /
# An empty string, or the contents of a file generated by the `htpasswd` command
k8s_container_htpasswd: ""
# List of dictionaries to mount the specified content as a data volume
# within the container:
# k8s_container_data_volumes:
#   - name: my-volume
#     mountPath: /path/to/dir
#     data:
#       file_name: content
k8s_container_data_volumes: []

k8s_web_containers:
  - name: "{{ k8s_container_name }}"
    image: "{{ k8s_container_image }}"
    image_pull_policy: "{{ k8s_container_image_pull_policy }}"
    tag: "{{ k8s_container_image_tag }}"
    replicas: "{{ k8s_container_replicas }}"
    resources: "{{ k8s_container_resources }}"
    port: "{{ k8s_container_port }}"
    protocol: "{{ k8s_container_protocol }}"
    ingress_annotations: "{{ k8s_container_ingress_annotations }}"
    ingress_paths: "{{ k8s_container_ingress_paths }}"
    environment: "{{ k8s_environment_variables }}"
    htpasswd: "{{ k8s_container_htpasswd }}"
    data_volumes: "{{ k8s_container_data_volumes }}"

k8s_worker_enabled: false
k8s_worker_name: "celery-worker"
k8s_worker_image: "{{ k8s_container_image }}"
k8s_worker_image_pull_policy: "{{ k8s_container_image_pull_policy }}"
k8s_worker_image_tag: "{{ k8s_container_image_tag }}"
k8s_worker_replicas: 2
k8s_worker_resources: "{{ k8s_container_resources }}"
k8s_worker_celery_app: ""
k8s_worker_command:
  - celery
  - --app={{ k8s_worker_celery_app }}
  - --workdir=/code
  - worker
  - --loglevel=info
k8s_worker_beat_command:
  - celery
  - --app={{ k8s_worker_celery_app }}
  - --workdir=/code
  - beat
  - --loglevel=info
  - --pidfile=/data/beat.pid
  - --schedule=/data/schedulefile.db
k8s_worker_data_volumes: "{{ k8s_container_data_volumes }}"
k8s_worker_container:
  name: "{{ k8s_worker_name }}"
  image: "{{ k8s_worker_image }}"
  image_pull_policy: "{{ k8s_worker_image_pull_policy }}"
  tag: "{{ k8s_worker_image_tag }}"
  replicas: "{{ k8s_worker_replicas }}"
  resources: "{{ k8s_worker_resources }}"
  celery_app: "{{ k8s_worker_celery_app }}"
  environment: "{{ k8s_environment_variables }}"
  worker_command: "{{ k8s_worker_command }}"
  beat_command: "{{ k8s_worker_beat_command }}"
  data_volumes: "{{ k8s_worker_data_volumes }}"
# Override k8s_worker_containers to add additional workers (background tasks):
k8s_worker_containers:
  - "{{ k8s_worker_container }}"

k8s_worker_beat_enabled: false

k8s_migrations_enabled: true
k8s_migrations_timeout: 120  # seconds
k8s_migration_command:  # DEPRECATED: keep name without trailing 's' for backwards compatibility
  - python
  - manage.py
  - migrate
  - --noinput
  - -v
  - "2"
k8s_migrations_command: "{{ k8s_migration_command }}"
k8s_migrations_batchjob:
  job_name: migrate
  batch_command: "{{ k8s_migrations_command }}"
  enabled: "{{ k8s_migrations_enabled }}"
  timeout: "{{ k8s_migrations_timeout }}"

k8s_collectstatic_enabled: true
k8s_collectstatic_timeout: 120  # seconds
k8s_collectstatic_command:
  - python
  - manage.py
  - collectstatic
  - --noinput
  - -v
  - "2"
k8s_collectstatic_batchjob:
  job_name: collectstatic
  batch_command: "{{ k8s_collectstatic_command }}"
  enabled: "{{ k8s_collectstatic_enabled }}"
  timeout: "{{ k8s_collectstatic_timeout }}"

# k8s_predeploy_batchjobs are one-off jobs that are run *before* every deploy. You can keep
# the default or override to customize the jobs needed for your project.
k8s_predeploy_batchjobs:
  - "{{ k8s_migrations_batchjob }}"
  - "{{ k8s_collectstatic_batchjob }}"
# k8s_postdeploy_batchjobs is a hook for running batch jobs *after* every deploy.
k8s_postdeploy_batchjobs: []
# k8s_batchjob_state is a hook used to delete job secrets if no batch jobs are enabled
k8s_batchjob_state: "{{ (k8s_predeploy_batchjobs + k8s_postdeploy_batchjobs) | map(attribute='enabled', default=true) | select | ternary('present', 'absent') }}"

k8s_s3_cluster_name: ""  # name of EKS cluster in AWS
k8s_s3_region: "us-east-1"
k8s_s3_namespace: "{{ k8s_namespace }}"
k8s_s3_iam_role: "S3ServiceAccountRole-{{ k8s_s3_namespace }}"
k8s_s3_serviceaccount: "default"
k8s_s3_public_bucket: "{{ k8s_s3_namespace }}-assets"
k8s_s3_private_bucket: "{{ k8s_s3_namespace }}-private-assets"
k8s_s3_tags: {}
k8s_s3_tags_purge: yes

k8s_templates:
  - name: registry_secret.yaml.j2
    state: "{{ k8s_dockerconfigjson | ternary('present', 'absent') }}"
  - name: redis.yaml.j2
    state: "{{ k8s_redis_enabled | ternary('present', 'absent') }}"
  - name: elasticsearch.yaml.j2
    state: "{{ k8s_elasticsearch_enabled | ternary('present', 'absent') }}"
  - name: memcached.yaml.j2
    state: "{{ k8s_memcached_enabled | ternary('present', 'absent') }}"
  - name: web.yaml.j2
    state: present
  - name: workers.yaml.j2
    state: "{{ k8s_worker_enabled | ternary('present', 'absent') }}"
  - name: workers_beat.yaml.j2
    state: "{{ (k8s_worker_enabled and k8s_worker_beat_enabled) | ternary('present', 'absent') }}"


# Note that this is only supported on AWS clusters
k8s_ci_create_user: "{{ k8s_ci_repository_arn | length > 0 }}"
k8s_ci_username: ci-user
k8s_ci_repository_arn: "" # format: arn:aws:ecr:<REGION>:<ACCOUNT_NUMBER>:repository/<REPO_NAME>
k8s_ci_vault_password_arn: "" # format: arn:aws:secretsmanager:<REGION>:<ACCOUNT_NUMBER:secret:<SECRET_IDENTIFIER>
